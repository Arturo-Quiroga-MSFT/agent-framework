{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdaab085",
   "metadata": {},
   "source": [
    "# Azure OpenAI Chat Completion Agent Demo\n",
    "\n",
    "This notebook demonstrates how to use Azure OpenAI Chat Completion Agents with the Microsoft Agent Framework.\n",
    "\n",
    "Based on: [Azure OpenAI Chat Completion Agents Documentation](https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/agent-types/azure-openai-chat-completion-agent?pivots=programming-language-python)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Azure CLI installed and authenticated (`az login`)\n",
    "2. Azure OpenAI resource created with a deployed model\n",
    "3. Environment variables set:\n",
    "   - `AZURE_OPENAI_ENDPOINT`\n",
    "   - `AZURE_OPENAI_CHAT_DEPLOYMENT_NAME`\n",
    "   - (Optional) `AZURE_OPENAI_API_VERSION`\n",
    "   - (Optional) `AZURE_OPENAI_API_KEY` (if not using Azure CLI authentication)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87f6782",
   "metadata": {},
   "source": [
    "## Setup: Load Environment Variables\n",
    "\n",
    "First, we'll load the required environment variables from the `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "425db0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Endpoint: https://aq-ai-foundry-sweden-central.openai.azure.com/\n",
      "Deployment Name: gpt-4.1\n",
      "API Version: Using default\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Navigate up from Notebooks -> agents -> getting_started -> samples -> python\n",
    "env_path = \"/Users/arturoquiroga/GITHUB/Agent-Framework-Private/python/.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Verify environment variables are loaded\n",
    "print(f\"OpenAI Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT', 'Not set')}\")\n",
    "print(f\"Deployment Name: {os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME', 'Not set')}\")\n",
    "print(f\"API Version: {os.getenv('AZURE_OPENAI_API_VERSION', 'Using default')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d30bc43",
   "metadata": {},
   "source": [
    "## Example 1: Basic Agent Creation\n",
    "\n",
    "The simplest way to create an agent using environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49850928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: The square root of 878 is approximately **29.649**.\n",
      "\n",
      "More precisely:  \n",
      "√878 ≈ **29.6486**\n"
     ]
    }
   ],
   "source": [
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "async def basic_agent_example():\n",
    "    \"\"\"Create and use a basic Azure OpenAI Chat Completion agent.\"\"\"\n",
    "    agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n",
    "        instructions=\"what is the square root of 878.\",\n",
    "        name=\"Joker\"\n",
    "    )\n",
    "\n",
    "    result = await agent.run(\"what is the square root of 878.\")\n",
    "    print(f\"Agent: {result.text}\")\n",
    "\n",
    "# Run the example\n",
    "await basic_agent_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a2f021",
   "metadata": {},
   "source": [
    "## Example 2: Explicit Configuration\n",
    "\n",
    "Instead of using environment variables, you can provide configuration explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4c13ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Sure! The Pythagorean theorem is a rule about right-angled triangles (triangles with one 90° angle).\n",
      "\n",
      "It says:\n",
      "\n",
      "**If you have a right triangle, the square of the length of the longest side (called the hypotenuse) is equal to the sum of the squares of the other two sides.**\n",
      "\n",
      "In math, it looks like this:\n",
      "\\[ a^2 + b^2 = c^2 \\]\n",
      "\n",
      "- **a** and **b** are the lengths of the two shorter sides,\n",
      "- **c** is the length of the longest side, which is opposite the right angle.\n",
      "\n",
      "**Example:**  \n",
      "If one side is 3 and the other is 4, then:\n",
      "\\( 3^2 + 4^2 = 9 + 16 = 25 \\)\n",
      "So the hypotenuse is \\( \\sqrt{25} = 5 \\).\n",
      "\n",
      "The theorem helps you find the length of a side if you know the other two!\n"
     ]
    }
   ],
   "source": [
    "async def explicit_config_example():\n",
    "    \"\"\"Create an agent with explicit configuration.\"\"\"\n",
    "    agent = AzureOpenAIChatClient(\n",
    "        endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "        credential=AzureCliCredential()\n",
    "    ).create_agent(\n",
    "        instructions=\"You are a helpful math tutor.\",\n",
    "        name=\"MathTutor\"\n",
    "    )\n",
    "\n",
    "    result = await agent.run(\"Explain the Pythagorean theorem in simple terms.\")\n",
    "    print(f\"Agent: {result.text}\")\n",
    "\n",
    "# Run the example\n",
    "await explicit_config_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403b833b",
   "metadata": {},
   "source": [
    "## Example 3: Streaming Responses\n",
    "\n",
    "Get responses as they are generated using streaming for better user experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d09d7d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Once upon a time in a misty green valley,Once upon a time in a misty green valley, there lived a brave knight named Sir Cedric. Clad in gleaming silver armor, Sir Cedric was known there lived a brave knight named Sir Cedric. Clad in gleaming silver armor, Sir Cedric was known across the land across the land for his unwavering courage and kind heart.\n",
      "\n",
      "One crisp morning, word spread that a mighty dragon had taken residence atop nearby Emberfire Hill for his unwavering courage and kind heart.\n",
      "\n",
      "One crisp morning, word spread that a mighty dragon had taken residence atop nearby Emberfire Hill. Fear gr. Fear gripped the villagers, for the beast's roar shook the windows and its shadow blotted out the sun.\n",
      "\n",
      "Without hesitationipped the villagers, for the beast's roar shook the windows and its shadow blotted out the sun.\n",
      "\n",
      "Without hesitation, Sir Cedric mounted his trusty horse, Sir Cedric mounted his trusty horse, Luna, and rode toward the hill. The climb was steep, the, Luna, and rode toward the hill. The climb was steep, the wind sharp like tiny daggers. At the summit, the dragon awaited— wind sharp like tiny daggers. At the summit, the dragon awaited—huge, with emerald scales andhuge, with emerald scales and eyes that glowed like lanterns in the gloom.\n",
      "\n",
      "Sir Cedric drew eyes that glowed like lanterns in the gloom.\n",
      "\n",
      "Sir Cedric drew his sword and, with a firm voice, called out, \"O his sword and, with a firm voice, called out, \"O mighty dragon, why do you trouble my people mighty dragon, why do you trouble my people?\"\n",
      "\n",
      "The dragon blinked, then spoke in a rumbling, sorrowful tone. \"I am alone and cold. The fire inside?\"\n",
      "\n",
      "The dragon blinked, then spoke in a rumbling, sorrowful tone. \"I am alone and cold. The fire inside me fades.\"\n",
      "\n",
      "Sir Cedric, seeing the sadness in the me fades.\"\n",
      "\n",
      "Sir Cedric, seeing the sadness in the dragon's eyes, lowered his sword. \"If you promise peace, I will dragon's eyes, lowered his sword. \"If you promise peace, I will help you help you,\" he said.\n",
      "\n",
      "Moved by Cedric's bravery and kindness, the,\" he said.\n",
      "\n",
      "Moved by Cedric's bravery and kindness, the dragon nodded. Together dragon nodded. Together, they gathered rare glowing moss from the caves below, known to restore a dragon's flame.\n",
      "\n",
      "With each handful, the dragon's fire rek, they gathered rare glowing moss from the caves below, known to restore a dragon's flame.\n",
      "\n",
      "With each handful, the dragon's fire rekindled, and he roaredindled, and he roared with gentle joy. From then on, the dragon kept Emberfire Hill safe with gentle joy. From then on, the dragon kept Emberfire Hill safe, and Sir Cedric and the villagers gained a powerful friend.\n",
      "\n",
      "And, and Sir Cedric and the villagers gained a powerful friend.\n",
      "\n",
      "And so, with bravery and so, with bravery and compassion, Sir Cedric turned an enemy into an ally, and the valley compassion, Sir Cedric turned an enemy into an ally, and the valley thrived in peace, watched over by a knight and thrived in peace, watched over by a knight and his dragon friend.\n",
      " his dragon friend.\n"
     ]
    }
   ],
   "source": [
    "async def streaming_example():\n",
    "    \"\"\"Stream responses as they are generated.\"\"\"\n",
    "    agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n",
    "        instructions=\"You are a creative storyteller.\",\n",
    "        name=\"StoryTeller\"\n",
    "    )\n",
    "\n",
    "    print(\"Agent: \", end=\"\", flush=True)\n",
    "    async for chunk in agent.run_stream(\"Tell me a short story about a brave knight and a dragon.\"):\n",
    "        if chunk.text:\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "    print()  # New line at the end\n",
    "\n",
    "# Run the example\n",
    "await streaming_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d92628",
   "metadata": {},
   "source": [
    "## Example 4: Function Tools - Single Function\n",
    "\n",
    "Provide custom function tools to Azure OpenAI agents. Functions can be automatically called by the agent when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ffc6664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: The current weather in Amsterdam is clear sky, with a temperature of 17.3°C (feels like 16.7°C). The humidity is 63%, and the wind speed is 6.17 m/s.\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from pydantic import Field\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# Import weather_utils from the azure_ai_real_weather directory (sibling directory)\n",
    "weather_utils_path = Path(\"/Users/arturoquiroga/GITHUB/Agent-Framework-Private/python/samples/getting_started/agents\") / \"azure_ai_real_weather\" / \"weather_utils.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"weather_utils\", weather_utils_path)\n",
    "weather_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(weather_utils)\n",
    "get_real_weather = weather_utils.get_real_weather\n",
    "\n",
    "async def function_tool_example():\n",
    "    \"\"\"Use a custom function tool with an agent.\"\"\"\n",
    "    agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        tools=get_real_weather\n",
    "    )\n",
    "\n",
    "    result = await agent.run(\"What is the weather like in Amsterdam?\")\n",
    "    print(f\"Agent: {result.text}\")\n",
    "\n",
    "# Run the example\n",
    "await function_tool_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84a31e",
   "metadata": {},
   "source": [
    "## Example 5: Function Tools with Decorator\n",
    "\n",
    "Use the `@ai_function` decorator to explicitly specify function name and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ebd005",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for weather_tool_input\nkwargs\n  Field required [type=missing, input_value={'chat_options': <agent_f...5db50>, 'args': 'Tokyo'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAgent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Run the example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m decorated_function_example()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mdecorated_function_example\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Use a decorated function tool.\"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n\u001b[32m     20\u001b[39m     instructions=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     tools=get_weather_decorated\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(\u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms the weather like in Tokyo?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAgent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_middleware.py:906\u001b[39m, in \u001b[36muse_agent_middleware.<locals>.middleware_enabled_run\u001b[39m\u001b[34m(self, messages, thread, middleware, **kwargs)\u001b[39m\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;28;01melse\u001b[39;00m AgentRunResponse()\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# No middleware, execute directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m original_run(\u001b[38;5;28mself\u001b[39m, normalized_messages, thread=thread, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/observability.py:996\u001b[39m, in \u001b[36m_trace_agent_run.<locals>.trace_run\u001b[39m\u001b[34m(self, messages, thread, **kwargs)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m OBSERVABILITY_SETTINGS\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OBSERVABILITY_SETTINGS.ENABLED:\n\u001b[32m    995\u001b[39m     \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_func(\u001b[38;5;28mself\u001b[39m, messages=messages, thread=thread, **kwargs)\n\u001b[32m    997\u001b[39m attributes = _get_span_attributes(\n\u001b[32m    998\u001b[39m     operation_name=OtelAttr.AGENT_INVOKE_OPERATION,\n\u001b[32m    999\u001b[39m     provider_name=provider_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m     **kwargs,\n\u001b[32m   1006\u001b[39m )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _get_span(attributes=attributes, span_name_attribute=OtelAttr.AGENT_NAME) \u001b[38;5;28;01mas\u001b[39;00m span:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_agents.py:556\u001b[39m, in \u001b[36mChatAgent.run\u001b[39m\u001b[34m(self, messages, thread, frequency_penalty, logit_bias, max_tokens, metadata, model, presence_penalty, response_format, seed, stop, store, temperature, tool_choice, tools, top_p, user, additional_properties, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_exit_stack.enter_async_context(mcp_server)\n\u001b[32m    555\u001b[39m     final_tools.extend(mcp_server.functions)\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chat_client.get_response(\n\u001b[32m    557\u001b[39m     messages=thread_messages,\n\u001b[32m    558\u001b[39m     chat_options=run_chat_options\n\u001b[32m    559\u001b[39m     & ChatOptions(\n\u001b[32m    560\u001b[39m         model_id=model,\n\u001b[32m    561\u001b[39m         conversation_id=thread.service_thread_id,\n\u001b[32m    562\u001b[39m         frequency_penalty=frequency_penalty,\n\u001b[32m    563\u001b[39m         logit_bias=logit_bias,\n\u001b[32m    564\u001b[39m         max_tokens=max_tokens,\n\u001b[32m    565\u001b[39m         metadata=metadata,\n\u001b[32m    566\u001b[39m         presence_penalty=presence_penalty,\n\u001b[32m    567\u001b[39m         response_format=response_format,\n\u001b[32m    568\u001b[39m         seed=seed,\n\u001b[32m    569\u001b[39m         stop=stop,\n\u001b[32m    570\u001b[39m         store=store,\n\u001b[32m    571\u001b[39m         temperature=temperature,\n\u001b[32m    572\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    573\u001b[39m         tools=final_tools,\n\u001b[32m    574\u001b[39m         top_p=top_p,\n\u001b[32m    575\u001b[39m         user=user,\n\u001b[32m    576\u001b[39m         additional_properties=additional_properties \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[32m    577\u001b[39m     ),\n\u001b[32m    578\u001b[39m     **kwargs,\n\u001b[32m    579\u001b[39m )\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._update_thread_with_type_and_conversation_id(thread, response.conversation_id)\n\u001b[32m    583\u001b[39m \u001b[38;5;66;03m# Ensure that the author name is set for each message in the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_tools.py:864\u001b[39m, in \u001b[36m_handle_function_calls_response.<locals>.decorator.<locals>.function_invocation_wrapper\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m function_calls \u001b[38;5;129;01mand\u001b[39;00m tools:\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# Use the stored middleware pipeline instead of extracting from kwargs\u001b[39;00m\n\u001b[32m    862\u001b[39m     \u001b[38;5;66;03m# because kwargs may have been modified by the underlying function\u001b[39;00m\n\u001b[32m    863\u001b[39m     middleware_pipeline = stored_middleware_pipeline\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m     function_call_results: \u001b[38;5;28mlist\u001b[39m[Contents] = \u001b[38;5;28;01mawait\u001b[39;00m execute_function_calls(\n\u001b[32m    865\u001b[39m         custom_args=kwargs,\n\u001b[32m    866\u001b[39m         attempt_idx=attempt_idx,\n\u001b[32m    867\u001b[39m         function_calls=function_calls,\n\u001b[32m    868\u001b[39m         tools=tools,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    869\u001b[39m         middleware_pipeline=middleware_pipeline,\n\u001b[32m    870\u001b[39m     )\n\u001b[32m    871\u001b[39m     \u001b[38;5;66;03m# add a single ChatMessage to the response with the results\u001b[39;00m\n\u001b[32m    872\u001b[39m     result_message = ChatMessage(role=\u001b[33m\"\u001b[39m\u001b[33mtool\u001b[39m\u001b[33m\"\u001b[39m, contents=function_call_results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_tools.py:780\u001b[39m, in \u001b[36mexecute_function_calls\u001b[39m\u001b[34m(custom_args, attempt_idx, function_calls, tools, middleware_pipeline)\u001b[39m\n\u001b[32m    778\u001b[39m tool_map = _get_tool_map(tools)\n\u001b[32m    779\u001b[39m \u001b[38;5;66;03m# Run all function calls concurrently\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m780\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*[\n\u001b[32m    781\u001b[39m     _auto_invoke_function(\n\u001b[32m    782\u001b[39m         function_call_content=function_call,\n\u001b[32m    783\u001b[39m         custom_args=custom_args,\n\u001b[32m    784\u001b[39m         tool_map=tool_map,\n\u001b[32m    785\u001b[39m         sequence_index=seq_idx,\n\u001b[32m    786\u001b[39m         request_index=attempt_idx,\n\u001b[32m    787\u001b[39m         middleware_pipeline=middleware_pipeline,\n\u001b[32m    788\u001b[39m     )\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m seq_idx, function_call \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(function_calls)\n\u001b[32m    790\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_tools.py:703\u001b[39m, in \u001b[36m_auto_invoke_function\u001b[39m\u001b[34m(function_call_content, custom_args, tool_map, sequence_index, request_index, middleware_pipeline)\u001b[39m\n\u001b[32m    701\u001b[39m \u001b[38;5;66;03m# Merge with user-supplied args; right-hand side dominates, so parsed args win on conflicts.\u001b[39;00m\n\u001b[32m    702\u001b[39m merged_args: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] = (custom_args \u001b[38;5;129;01mor\u001b[39;00m {}) | parsed_args\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m args = \u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m exception = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m \u001b[38;5;66;03m# Execute through middleware pipeline if available\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/pydantic/main.py:705\u001b[39m, in \u001b[36mBaseModel.model_validate\u001b[39m\u001b[34m(cls, obj, strict, from_attributes, context, by_alias, by_name)\u001b[39m\n\u001b[32m    699\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m by_alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PydanticUserError(\n\u001b[32m    701\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mAt least one of `by_alias` or `by_name` must be set to True.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    702\u001b[39m         code=\u001b[33m'\u001b[39m\u001b[33mvalidate-by-alias-and-name-false\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    703\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m705\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_alias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mby_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_name\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for weather_tool_input\nkwargs\n  Field required [type=missing, input_value={'chat_options': <agent_f...5db50>, 'args': 'Tokyo'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/missing"
     ]
    }
   ],
   "source": [
    "from agent_framework import ai_function\n",
    "import importlib.util\n",
    "from pathlib import Path\n",
    "\n",
    "# Import weather_utils from the azure_ai_real_weather directory (sibling directory)\n",
    "weather_utils_path = Path(\"/Users/arturoquiroga/GITHUB/Agent-Framework-Private/python/samples/getting_started/agents\") / \"azure_ai_real_weather\" / \"weather_utils.py\"\n",
    "spec = importlib.util.spec_from_file_location(\"weather_utils\", weather_utils_path)\n",
    "weather_utils = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(weather_utils)\n",
    "get_real_weather = weather_utils.get_real_weather\n",
    "\n",
    "# Decorate the get_real_weather function\n",
    "@ai_function(name=\"weather_tool\", description=\"Retrieves weather information for any location\")\n",
    "def get_weather_decorated(*args, **kwargs):\n",
    "    return get_real_weather(*args, **kwargs)\n",
    "\n",
    "async def decorated_function_example():\n",
    "    \"\"\"Use a decorated function tool.\"\"\"\n",
    "    agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        tools=get_weather_decorated\n",
    "    )\n",
    "\n",
    "    result = await agent.run(\"What's the weather like in Tokyo?\")\n",
    "    print(f\"Agent: {result.text}\")\n",
    "\n",
    "# Run the example\n",
    "await decorated_function_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0d8635",
   "metadata": {},
   "source": [
    "## Example 6: Multiple Function Tools\n",
    "\n",
    "Provide multiple function tools to an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66875603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Here are your requested results:\n",
      "\n",
      "1. The current time is 10:41:39.\n",
      "2. You rolled an 18 on a 20-sided dice!\n",
      "3. An 18% tip on a $75.50 bill is $13.59, making the total $89.09.\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "from datetime import datetime\n",
    "\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current time.\"\"\"\n",
    "    return f\"The current time is {datetime.now().strftime('%H:%M:%S')}.\"\n",
    "\n",
    "def roll_dice(\n",
    "    sides: Annotated[int, Field(description=\"Number of sides on the dice\", ge=4, le=100)] = 6\n",
    ") -> str:\n",
    "    \"\"\"Roll a dice with the specified number of sides.\"\"\"\n",
    "    result = randint(1, sides)\n",
    "    return f\"You rolled a {result} on a {sides}-sided dice!\"\n",
    "\n",
    "def calculate_tip(\n",
    "    bill_amount: Annotated[float, Field(description=\"The total bill amount\")],\n",
    "    tip_percentage: Annotated[float, Field(description=\"Tip percentage (e.g., 15 for 15%)\")]\n",
    ") -> str:\n",
    "    \"\"\"Calculate the tip amount and total bill.\"\"\"\n",
    "    tip_amount = bill_amount * (tip_percentage / 100)\n",
    "    total = bill_amount + tip_amount\n",
    "    return f\"Tip amount: ${tip_amount:.2f}, Total: ${total:.2f}\"\n",
    "\n",
    "async def multiple_tools_example():\n",
    "    \"\"\"Use multiple function tools together.\"\"\"\n",
    "    agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n",
    "        instructions=\"You are a helpful assistant.\",\n",
    "        tools=[get_current_time, roll_dice, calculate_tip, get_real_weather]\n",
    "    )\n",
    "\n",
    "    # Query that might use multiple tools\n",
    "    result = await agent.run(\n",
    "        \"First, tell me the current time. Then roll a 20-sided dice. \"\n",
    "        \"Finally, calculate a 18% tip on a $75.50 bill.\"\n",
    "    )\n",
    "    print(f\"Agent: {result.text}\")\n",
    "\n",
    "# Run the example\n",
    "await multiple_tools_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a516c2",
   "metadata": {},
   "source": [
    "## Example 7: Function Tools as a Class\n",
    "\n",
    "Organize related functions together in a class. This is useful for sharing state between functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07888afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: The current weather in Miami is sunny with a temperature of 28°C.\n",
      "\n",
      "The detailed forecast for tomorrow in Miami shows similar sunny conditions are expected.\n"
     ]
    }
   ],
   "source": [
    "class WeatherTools:\n",
    "    \"\"\"A collection of weather-related tools.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simulated weather cache\n",
    "        self.weather_cache = {\n",
    "            \"Seattle\": \"Rainy, 12°C\",\n",
    "            \"Miami\": \"Sunny, 28°C\",\n",
    "            \"London\": \"Cloudy, 15°C\",\n",
    "            \"Sydney\": \"Partly cloudy, 22°C\"\n",
    "        }\n",
    "    \n",
    "    def get_weather(self, location: Annotated[str, Field(description=\"The location to get the weather for.\")]) -> str:\n",
    "        \"\"\"Get the current weather for a location.\"\"\"\n",
    "        return f\"The weather in {location} is {self.weather_cache.get(location, 'sunny with a high of 20°C')}.\"\n",
    "    \n",
    "    def get_weather_details(\n",
    "        self,\n",
    "        location: Annotated[str, Field(description=\"The location to get detailed weather for.\")]\n",
    "    ) -> str:\n",
    "        \"\"\"Get detailed weather information including forecast.\"\"\"\n",
    "        base_weather = self.weather_cache.get(location, \"sunny, 20°C\")\n",
    "        return f\"Current weather in {location}: {base_weather}. Tomorrow's forecast: Similar conditions expected.\"\n",
    "\n",
    "async def class_tools_example():\n",
    "    \"\"\"Use function tools from a class.\"\"\"\n",
    "    tools = WeatherTools()\n",
    "    \n",
    "    agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n",
    "        instructions=\"You are a helpful weather assistant.\",\n",
    "        tools=[tools.get_weather, tools.get_weather_details]\n",
    "    )\n",
    "\n",
    "    result = await agent.run(\"What's the weather like in Miami? Also give me the detailed forecast.\")\n",
    "    print(f\"Agent: {result.text}\")\n",
    "\n",
    "# Run the example\n",
    "await class_tools_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3e3bb7",
   "metadata": {},
   "source": [
    "## Example 8: Conversation with Memory (Using ChatAgent)\n",
    "\n",
    "Use `ChatAgent` wrapper for more control and conversation memory across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24df5d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: My name is Alex and I'm planning a trip to Paris.\n",
      "Agent: Nice to meet you, Alex! Paris is an incredible destination with so much to see and do. How can I assist you with your trip planning? Are you looking for things to do, travel tips, hotel or restaurant recommendations, or perhaps current weather information? Let me know how I can help make your trip amazing!\n",
      "\n",
      "User: What's my name and where am I going?\n",
      "Agent: Nice to meet you, Alex! Paris is an incredible destination with so much to see and do. How can I assist you with your trip planning? Are you looking for things to do, travel tips, hotel or restaurant recommendations, or perhaps current weather information? Let me know how I can help make your trip amazing!\n",
      "\n",
      "User: What's my name and where am I going?\n",
      "Agent: Your name is Alex, and you are planning a trip to Paris. If you need help with any details or have any questions about your trip, just let me know!\n",
      "\n",
      "User: Can you check the weather for my destination?\n",
      "Agent: Your name is Alex, and you are planning a trip to Paris. If you need help with any details or have any questions about your trip, just let me know!\n",
      "\n",
      "User: Can you check the weather for my destination?\n"
     ]
    },
    {
     "ename": "ServiceResponseException",
     "evalue": "<class 'agent_framework.azure._chat_client.AzureOpenAIChatClient'> service failed to complete the prompt: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/openai/_chat_client.py:72\u001b[39m, in \u001b[36mOpenAIBaseChatClient._inner_get_response\u001b[39m\u001b[34m(self, messages, chat_options, **kwargs)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_response(\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat.completions.create(stream=\u001b[38;5;28;01mFalse\u001b[39;00m, **options_dict), chat_options\n\u001b[32m     73\u001b[39m     )\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequestError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2585\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2584\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2585\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2587\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2588\u001b[39m         {\n\u001b[32m   2589\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2590\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2591\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2592\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2593\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2594\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2595\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2596\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2597\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2598\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2599\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2600\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2601\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2602\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2603\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2604\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2605\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2606\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2607\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2608\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2609\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2610\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2611\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2612\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2613\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2614\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2615\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2616\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2617\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2619\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2620\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2621\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2622\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2623\u001b[39m         },\n\u001b[32m   2624\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2625\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2626\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2627\u001b[39m     ),\n\u001b[32m   2628\u001b[39m     options=make_request_options(\n\u001b[32m   2629\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2630\u001b[39m     ),\n\u001b[32m   2631\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2632\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2633\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2634\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/openai/_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/openai/_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1593\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mServiceResponseException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAgent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult3.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Run the example\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m conversation_memory_example()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mconversation_memory_example\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Third turn - check weather using the tool\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUser: Can you check the weather for my destination?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m result3 = \u001b[38;5;28;01mawait\u001b[39;00m agent.run(\n\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCan you check the weather for my destination?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     41\u001b[39m     thread=thread,\n\u001b[32m     42\u001b[39m     store=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     43\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAgent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult3.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_middleware.py:906\u001b[39m, in \u001b[36muse_agent_middleware.<locals>.middleware_enabled_run\u001b[39m\u001b[34m(self, messages, thread, middleware, **kwargs)\u001b[39m\n\u001b[32m    903\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;28;01melse\u001b[39;00m AgentRunResponse()\n\u001b[32m    905\u001b[39m \u001b[38;5;66;03m# No middleware, execute directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m906\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m original_run(\u001b[38;5;28mself\u001b[39m, normalized_messages, thread=thread, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/observability.py:996\u001b[39m, in \u001b[36m_trace_agent_run.<locals>.trace_run\u001b[39m\u001b[34m(self, messages, thread, **kwargs)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m OBSERVABILITY_SETTINGS\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OBSERVABILITY_SETTINGS.ENABLED:\n\u001b[32m    995\u001b[39m     \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m996\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_func(\u001b[38;5;28mself\u001b[39m, messages=messages, thread=thread, **kwargs)\n\u001b[32m    997\u001b[39m attributes = _get_span_attributes(\n\u001b[32m    998\u001b[39m     operation_name=OtelAttr.AGENT_INVOKE_OPERATION,\n\u001b[32m    999\u001b[39m     provider_name=provider_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1005\u001b[39m     **kwargs,\n\u001b[32m   1006\u001b[39m )\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _get_span(attributes=attributes, span_name_attribute=OtelAttr.AGENT_NAME) \u001b[38;5;28;01mas\u001b[39;00m span:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_agents.py:556\u001b[39m, in \u001b[36mChatAgent.run\u001b[39m\u001b[34m(self, messages, thread, frequency_penalty, logit_bias, max_tokens, metadata, model, presence_penalty, response_format, seed, stop, store, temperature, tool_choice, tools, top_p, user, additional_properties, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_exit_stack.enter_async_context(mcp_server)\n\u001b[32m    555\u001b[39m     final_tools.extend(mcp_server.functions)\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chat_client.get_response(\n\u001b[32m    557\u001b[39m     messages=thread_messages,\n\u001b[32m    558\u001b[39m     chat_options=run_chat_options\n\u001b[32m    559\u001b[39m     & ChatOptions(\n\u001b[32m    560\u001b[39m         model_id=model,\n\u001b[32m    561\u001b[39m         conversation_id=thread.service_thread_id,\n\u001b[32m    562\u001b[39m         frequency_penalty=frequency_penalty,\n\u001b[32m    563\u001b[39m         logit_bias=logit_bias,\n\u001b[32m    564\u001b[39m         max_tokens=max_tokens,\n\u001b[32m    565\u001b[39m         metadata=metadata,\n\u001b[32m    566\u001b[39m         presence_penalty=presence_penalty,\n\u001b[32m    567\u001b[39m         response_format=response_format,\n\u001b[32m    568\u001b[39m         seed=seed,\n\u001b[32m    569\u001b[39m         stop=stop,\n\u001b[32m    570\u001b[39m         store=store,\n\u001b[32m    571\u001b[39m         temperature=temperature,\n\u001b[32m    572\u001b[39m         tool_choice=tool_choice,\n\u001b[32m    573\u001b[39m         tools=final_tools,\n\u001b[32m    574\u001b[39m         top_p=top_p,\n\u001b[32m    575\u001b[39m         user=user,\n\u001b[32m    576\u001b[39m         additional_properties=additional_properties \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[32m    577\u001b[39m     ),\n\u001b[32m    578\u001b[39m     **kwargs,\n\u001b[32m    579\u001b[39m )\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._update_thread_with_type_and_conversation_id(thread, response.conversation_id)\n\u001b[32m    583\u001b[39m \u001b[38;5;66;03m# Ensure that the author name is set for each message in the response.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_tools.py:842\u001b[39m, in \u001b[36m_handle_function_calls_response.<locals>.decorator.<locals>.function_invocation_wrapper\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    840\u001b[39m fcc_messages: \u001b[33m\"\u001b[39m\u001b[33mlist[ChatMessage]\u001b[39m\u001b[33m\"\u001b[39m = []\n\u001b[32m    841\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m attempt_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iterations):\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, messages=prepped_messages, **kwargs)\n\u001b[32m    843\u001b[39m     \u001b[38;5;66;03m# if there are function calls, we will handle them first\u001b[39;00m\n\u001b[32m    844\u001b[39m     function_results = {\n\u001b[32m    845\u001b[39m         it.call_id \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m response.messages[\u001b[32m0\u001b[39m].contents \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(it, FunctionResultContent)\n\u001b[32m    846\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/observability.py:771\u001b[39m, in \u001b[36m_trace_get_response.<locals>.decorator.<locals>.trace_get_response\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m    768\u001b[39m \u001b[38;5;28;01mglobal\u001b[39;00m OBSERVABILITY_SETTINGS\n\u001b[32m    769\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m OBSERVABILITY_SETTINGS.ENABLED:\n\u001b[32m    770\u001b[39m     \u001b[38;5;66;03m# If model diagnostics are not enabled, just return the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(\n\u001b[32m    772\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    773\u001b[39m         messages=messages,\n\u001b[32m    774\u001b[39m         **kwargs,\n\u001b[32m    775\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mtoken_usage_histogram\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.additional_properties:\n\u001b[32m    777\u001b[39m     \u001b[38;5;28mself\u001b[39m.additional_properties[\u001b[33m\"\u001b[39m\u001b[33mtoken_usage_histogram\u001b[39m\u001b[33m\"\u001b[39m] = _get_token_usage_histogram()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_middleware.py:1003\u001b[39m, in \u001b[36muse_chat_middleware.<locals>.middleware_enabled_get_response\u001b[39m\u001b[34m(self, messages, **kwargs)\u001b[39m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# If no chat middleware, use original method\u001b[39;00m\n\u001b[32m   1002\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m chat_middleware_list:\n\u001b[32m-> \u001b[39m\u001b[32m1003\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m original_get_response(\u001b[38;5;28mself\u001b[39m, messages, **kwargs)\n\u001b[32m   1005\u001b[39m \u001b[38;5;66;03m# Create pipeline and execute with middleware\u001b[39;00m\n\u001b[32m   1006\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOptions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/_clients.py:416\u001b[39m, in \u001b[36mBaseChatClient.get_response\u001b[39m\u001b[34m(self, messages, frequency_penalty, logit_bias, max_tokens, metadata, model, presence_penalty, response_format, seed, stop, store, temperature, tool_choice, tools, top_p, user, additional_properties, **kwargs)\u001b[39m\n\u001b[32m    413\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_tool_choice(chat_options=chat_options)\n\u001b[32m    415\u001b[39m filtered_kwargs = \u001b[38;5;28mself\u001b[39m._filter_internal_kwargs(kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._inner_get_response(messages=prepped_messages, chat_options=chat_options, **filtered_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/GITHUB/Agent-Framework-Private/.venv/lib/python3.13/site-packages/agent_framework/openai/_chat_client.py:80\u001b[39m, in \u001b[36mOpenAIBaseChatClient._inner_get_response\u001b[39m\u001b[34m(self, messages, chat_options, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ex.code == \u001b[33m\"\u001b[39m\u001b[33mcontent_filter\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m OpenAIContentFilterException(\n\u001b[32m     77\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service encountered a content error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     78\u001b[39m             inner_exception=ex,\n\u001b[32m     79\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m     81\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     82\u001b[39m         inner_exception=ex,\n\u001b[32m     83\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ServiceResponseException(\n\u001b[32m     86\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m service failed to complete the prompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mex\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     87\u001b[39m         inner_exception=ex,\n\u001b[32m     88\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n",
      "\u001b[31mServiceResponseException\u001b[39m: <class 'agent_framework.azure._chat_client.AzureOpenAIChatClient'> service failed to complete the prompt: Error code: 400 - {'error': {'message': \"Invalid parameter: messages with role 'tool' must be a response to a preceeding message with 'tool_calls'.\", 'type': 'invalid_request_error', 'param': 'messages.[2].role', 'code': None}}"
     ]
    }
   ],
   "source": [
    "from agent_framework import ChatAgent\n",
    "\n",
    "async def conversation_memory_example():\n",
    "    \"\"\"Demonstrate conversation memory with multiple turns.\"\"\"\n",
    "    # Create a chat client\n",
    "    chat_client = AzureOpenAIChatClient(credential=AzureCliCredential())\n",
    "    \n",
    "    # Create a chat agent with the client\n",
    "    agent = ChatAgent(\n",
    "        chat_client=chat_client,\n",
    "        name=\"MemoryAgent\",\n",
    "        instructions=\"You are a helpful assistant that remembers conversation context.\",\n",
    "        tools=get_real_weather\n",
    "    )\n",
    "    \n",
    "    # Create a thread to maintain conversation history\n",
    "    thread = agent.get_new_thread()\n",
    "    \n",
    "    # First turn\n",
    "    print(\"User: My name is Alex and I'm planning a trip to Paris.\")\n",
    "    result1 = await agent.run(\n",
    "        \"My name is Alex and I'm planning a trip to Paris.\",\n",
    "        thread=thread,\n",
    "        store=True\n",
    "    )\n",
    "    print(f\"Agent: {result1.text}\\n\")\n",
    "    \n",
    "    # Second turn - agent should remember the name and destination\n",
    "    print(\"User: What's my name and where am I going?\")\n",
    "    result2 = await agent.run(\n",
    "        \"What's my name and where am I going?\",\n",
    "        thread=thread,\n",
    "        store=True\n",
    "    )\n",
    "    print(f\"Agent: {result2.text}\\n\")\n",
    "    \n",
    "    # Third turn - check weather using the tool\n",
    "    print(\"User: Can you check the weather for my destination?\")\n",
    "    result3 = await agent.run(\n",
    "        \"Can you check the weather for my destination?\",\n",
    "        thread=thread,\n",
    "        store=True\n",
    "    )\n",
    "    print(f\"Agent: {result3.text}\")\n",
    "\n",
    "# Run the example\n",
    "await conversation_memory_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b75f2",
   "metadata": {},
   "source": [
    "## Example 9: Streaming with Function Calls\n",
    "\n",
    "Combine streaming responses with function tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d01e756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a short poem inspired by today's events:\n",
      "\n",
      "In Seattle's sky, a short poem inspired by today's events:\n",
      "\n",
      "In Seattle's sky, the clouds all roam,\n",
      "Broken and drifting, far from home.\n",
      "A the clouds all roam,\n",
      "Broken and drifting, far from home.\n",
      "A gentle breeze, cool and profound gentle breeze, cool and profound,\n",
      "While temperatures hover, calm all around.\n",
      "\n",
      "A dice was rolled, and luck,\n",
      "While temperatures hover, calm all around.\n",
      "\n",
      "A dice was rolled, and luck did mix,\n",
      "It spun and landed straight on six!\n",
      "Weather or did mix,\n",
      "It spun and landed straight on six!\n",
      "Weather or chance, both have their chance, both have their say,\n",
      "Bringing a little magic to this day.\n",
      " say,\n",
      "Bringing a little magic to this day.\n"
     ]
    }
   ],
   "source": [
    "async def streaming_with_tools_example():\n",
    "    \"\"\"Stream responses while using function tools.\"\"\"\n",
    "    agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n",
    "        instructions=\"You are a helpful assistant. When providing weather information, always use the available tool.\",\n",
    "        tools=[get_real_weather, roll_dice]\n",
    "    )\n",
    "\n",
    "    print(\"Agent: \", end=\"\", flush=True)\n",
    "    async for chunk in agent.run_stream(\n",
    "        \"Check the weather in Seattle, then roll a dice and write a short poem about both results.\"\n",
    "    ):\n",
    "        if chunk.text:\n",
    "            print(chunk.text, end=\"\", flush=True)\n",
    "    print()  # New line at the end\n",
    "\n",
    "# Run the example\n",
    "await streaming_with_tools_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c50293",
   "metadata": {},
   "source": [
    "## Example 10: Error Handling\n",
    "\n",
    "Demonstrate proper error handling when working with agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c451f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Hello! I’m just a computer program, but I’m here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "async def error_handling_example():\n",
    "    \"\"\"Demonstrate error handling with agents.\"\"\"\n",
    "    try:\n",
    "        agent = AzureOpenAIChatClient(credential=AzureCliCredential()).create_agent(\n",
    "            instructions=\"You are a helpful assistant.\",\n",
    "            name=\"ErrorDemo\"\n",
    "        )\n",
    "        \n",
    "        result = await agent.run(\"Hello, how are you?\")\n",
    "        print(f\"Success: {result.text}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {type(e).__name__}: {str(e)}\")\n",
    "        print(\"\\nMake sure your environment variables are set correctly:\")\n",
    "        print(\"- AZURE_OPENAI_ENDPOINT\")\n",
    "        print(\"- AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "        print(\"\\nAnd that you've run 'az login' for authentication.\")\n",
    "\n",
    "# Run the example\n",
    "await error_handling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0b626c",
   "metadata": {},
   "source": [
    "## Example 11: Advanced - Custom Chat Client Configuration\n",
    "\n",
    "Configure the chat client with additional parameters like temperature, max tokens, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c328e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Suspended amid clouds of neon mist, the city spirals skyward on living glass tendrils, where bioluminescent gardens bloom upon gravity-defying terraces and sentient drones sing lullabies through the ever-shifting, holographic skyline.\n"
     ]
    }
   ],
   "source": [
    "async def advanced_config_example():\n",
    "    \"\"\"Use advanced configuration options.\"\"\"\n",
    "    # Create a chat client with custom settings\n",
    "    chat_client = AzureOpenAIChatClient(\n",
    "        endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        deployment_name=os.environ[\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"],\n",
    "        credential=AzureCliCredential(),\n",
    "        # Note: Additional parameters like temperature are typically set per-request\n",
    "        # rather than on the client itself\n",
    "    )\n",
    "    \n",
    "    agent = chat_client.create_agent(\n",
    "        instructions=\"You are a creative writer. Be very creative and imaginative.\",\n",
    "        name=\"CreativeWriter\"\n",
    "    )\n",
    "\n",
    "    result = await agent.run(\"Write a one-sentence description of a futuristic city.\")\n",
    "    print(f\"Agent: {result.text}\")\n",
    "\n",
    "# Run the example\n",
    "await advanced_config_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a5a14",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated key features of Azure OpenAI Chat Completion Agents:\n",
    "\n",
    "1. **Basic Agent Creation** - Simple agent setup with environment variables\n",
    "2. **Explicit Configuration** - Direct configuration without environment variables\n",
    "3. **Streaming Responses** - Real-time response generation\n",
    "4. **Single Function Tool** - Custom function as a tool\n",
    "5. **Decorated Functions** - Using `@ai_function` decorator\n",
    "6. **Multiple Function Tools** - Combining multiple tools\n",
    "7. **Class-based Tools** - Organizing related functions in a class\n",
    "8. **Conversation Memory** - Maintaining context with ChatAgent and threads\n",
    "9. **Streaming with Tools** - Combining streaming and function calls\n",
    "10. **Error Handling** - Proper exception handling\n",
    "11. **Advanced Configuration** - Custom client settings\n",
    "\n",
    "## Key Differences from Azure AI Foundry Agents\n",
    "\n",
    "- **No persistent storage**: Chat completion agents don't persist to a service\n",
    "- **Custom history storage**: You manage conversation history yourself\n",
    "- **More flexible**: Direct access to Azure OpenAI Chat Completion API\n",
    "- **Lighter weight**: No need for Azure AI Foundry project setup\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore [Agent Framework Tutorials](https://learn.microsoft.com/en-us/agent-framework/tutorials/overview)\n",
    "- Learn about [Azure AI Foundry Agents](https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/agent-types/azure-ai-foundry-agent)\n",
    "- Try [OpenAI Chat Completion Agents](https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/agent-types/openai-chat-completion-agent)\n",
    "- Check out more examples in the `samples` directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
